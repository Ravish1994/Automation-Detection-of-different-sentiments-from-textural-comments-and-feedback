{"cells":[{"source":"<a href=\"https://www.kaggle.com/raviprakashkumar/automation-detection-of-different-sentiments-from?scriptVersionId=84386671\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown","outputs":[],"execution_count":0},{"cell_type":"markdown","id":"e44976c1","metadata":{"papermill":{"duration":0.022118,"end_time":"2022-01-04T11:27:37.380978","exception":false,"start_time":"2022-01-04T11:27:37.35886","status":"completed"},"tags":[]},"source":["### Algorithm applied : \n","+ NLP(Natural Language Processing)"]},{"cell_type":"markdown","id":"c2ae2362","metadata":{"papermill":{"duration":0.021484,"end_time":"2022-01-04T11:27:37.423082","exception":false,"start_time":"2022-01-04T11:27:37.401598","status":"completed"},"tags":[]},"source":["### Project Objective : \n","+ To develop a deep learning algorithm to detect different types of sentiments contained in a collection of English sentences or a large paragraph.   "]},{"cell_type":"markdown","id":"f7bc244e","metadata":{"papermill":{"duration":0.019556,"end_time":"2022-01-04T11:27:37.464067","exception":false,"start_time":"2022-01-04T11:27:37.444511","status":"completed"},"tags":[]},"source":["### To predict the number of positive and negative reviews using either classification or deep learning algorithms."]},{"cell_type":"markdown","id":"0a279cbc","metadata":{"papermill":{"duration":0.02041,"end_time":"2022-01-04T11:27:37.504812","exception":false,"start_time":"2022-01-04T11:27:37.484402","status":"completed"},"tags":[]},"source":["### Importing Libraries"]},{"cell_type":"code","execution_count":1,"id":"49936f61","metadata":{"execution":{"iopub.execute_input":"2022-01-04T11:27:37.556082Z","iopub.status.busy":"2022-01-04T11:27:37.555031Z","iopub.status.idle":"2022-01-04T11:27:37.560542Z","shell.execute_reply":"2022-01-04T11:27:37.561038Z","shell.execute_reply.started":"2022-01-04T11:19:04.077308Z"},"papermill":{"duration":0.035644,"end_time":"2022-01-04T11:27:37.56141","exception":false,"start_time":"2022-01-04T11:27:37.525766","status":"completed"},"tags":[]},"outputs":[],"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","import pandas as pd"]},{"cell_type":"markdown","id":"40f62f66","metadata":{"papermill":{"duration":0.021315,"end_time":"2022-01-04T11:27:37.603206","exception":false,"start_time":"2022-01-04T11:27:37.581891","status":"completed"},"tags":[]},"source":["### Importing datasets"]},{"cell_type":"markdown","id":"ad1f6997","metadata":{"papermill":{"duration":0.019803,"end_time":"2022-01-04T11:27:37.643795","exception":false,"start_time":"2022-01-04T11:27:37.623992","status":"completed"},"tags":[]},"source":["+ IMDB dataset having 50K movie reviews for natural language processing or Text analytics. This is a dataset for binary sentiment classification containing substantially more data than previous benchmark datasets. It provide a set of 25,000 highly polar movie reviews for training and 25,000 for testing. \n","+ Kaggle datasets: http://ai.stanford.edu/~amaas/data/sentiment/"]},{"cell_type":"code","execution_count":2,"id":"a08556bf","metadata":{"execution":{"iopub.execute_input":"2022-01-04T11:27:37.689248Z","iopub.status.busy":"2022-01-04T11:27:37.688504Z","iopub.status.idle":"2022-01-04T11:27:39.54411Z","shell.execute_reply":"2022-01-04T11:27:39.544604Z","shell.execute_reply.started":"2022-01-04T11:19:04.108069Z"},"papermill":{"duration":1.881021,"end_time":"2022-01-04T11:27:39.544795","exception":false,"start_time":"2022-01-04T11:27:37.663774","status":"completed"},"tags":[]},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>review</th>\n","      <th>sentiment</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>One of the other reviewers has mentioned that ...</td>\n","      <td>positive</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n","      <td>positive</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>I thought this was a wonderful way to spend ti...</td>\n","      <td>positive</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>Basically there's a family where a little boy ...</td>\n","      <td>negative</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n","      <td>positive</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>49995</th>\n","      <td>I thought this movie did a down right good job...</td>\n","      <td>positive</td>\n","    </tr>\n","    <tr>\n","      <th>49996</th>\n","      <td>Bad plot, bad dialogue, bad acting, idiotic di...</td>\n","      <td>negative</td>\n","    </tr>\n","    <tr>\n","      <th>49997</th>\n","      <td>I am a Catholic taught in parochial elementary...</td>\n","      <td>negative</td>\n","    </tr>\n","    <tr>\n","      <th>49998</th>\n","      <td>I'm going to have to disagree with the previou...</td>\n","      <td>negative</td>\n","    </tr>\n","    <tr>\n","      <th>49999</th>\n","      <td>No one expects the Star Trek movies to be high...</td>\n","      <td>negative</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>50000 rows × 2 columns</p>\n","</div>"],"text/plain":["                                                  review sentiment\n","0      One of the other reviewers has mentioned that ...  positive\n","1      A wonderful little production. <br /><br />The...  positive\n","2      I thought this was a wonderful way to spend ti...  positive\n","3      Basically there's a family where a little boy ...  negative\n","4      Petter Mattei's \"Love in the Time of Money\" is...  positive\n","...                                                  ...       ...\n","49995  I thought this movie did a down right good job...  positive\n","49996  Bad plot, bad dialogue, bad acting, idiotic di...  negative\n","49997  I am a Catholic taught in parochial elementary...  negative\n","49998  I'm going to have to disagree with the previou...  negative\n","49999  No one expects the Star Trek movies to be high...  negative\n","\n","[50000 rows x 2 columns]"]},"execution_count":2,"metadata":{},"output_type":"execute_result"}],"source":["dataset = pd.read_csv('../input/imdb-dataset-of-50k-movie-reviews/IMDB Dataset.csv')\n","dataset"]},{"cell_type":"code","execution_count":3,"id":"39c3e985","metadata":{"execution":{"iopub.execute_input":"2022-01-04T11:27:39.592064Z","iopub.status.busy":"2022-01-04T11:27:39.591418Z","iopub.status.idle":"2022-01-04T11:27:39.596714Z","shell.execute_reply":"2022-01-04T11:27:39.597212Z","shell.execute_reply.started":"2022-01-04T11:19:05.850514Z"},"papermill":{"duration":0.029253,"end_time":"2022-01-04T11:27:39.597379","exception":false,"start_time":"2022-01-04T11:27:39.568126","status":"completed"},"tags":[]},"outputs":[{"data":{"text/plain":["Index(['review', 'sentiment'], dtype='object')"]},"execution_count":3,"metadata":{},"output_type":"execute_result"}],"source":["dataset.keys()"]},{"cell_type":"markdown","id":"f1f11b13","metadata":{"papermill":{"duration":0.020716,"end_time":"2022-01-04T11:27:39.640432","exception":false,"start_time":"2022-01-04T11:27:39.619716","status":"completed"},"tags":[]},"source":["### Cleaning the text\n","+ We are going to remove punctuations and different types of special characters, capital letters or lower case letters from our text as these things will create problem while processing the data"]},{"cell_type":"code","execution_count":4,"id":"d87922c3","metadata":{"execution":{"iopub.execute_input":"2022-01-04T11:27:39.685885Z","iopub.status.busy":"2022-01-04T11:27:39.685237Z","iopub.status.idle":"2022-01-04T11:28:01.384068Z","shell.execute_reply":"2022-01-04T11:28:01.38464Z","shell.execute_reply.started":"2022-01-04T11:19:05.857775Z"},"papermill":{"duration":21.723347,"end_time":"2022-01-04T11:28:01.384845","exception":false,"start_time":"2022-01-04T11:27:39.661498","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["[nltk_data] Error loading stopwords: <urlopen error [Errno -3]\n","[nltk_data]     Temporary failure in name resolution>\n"]}],"source":["import re\n","# Importing library to remove stopwords from our text as it will not help to predict our text emotion \n","# like all the articles(the, a , an...)\n","import nltk\n","\n","# Downloading all the stopwords from the nltk library\n","nltk.download('stopwords')\n","\n","# Importing stopwords\n","from nltk.corpus import stopwords"]},{"cell_type":"markdown","id":"9deaae2c","metadata":{"papermill":{"duration":0.021255,"end_time":"2022-01-04T11:28:01.427512","exception":false,"start_time":"2022-01-04T11:28:01.406257","status":"completed"},"tags":[]},"source":["### Stemming of Text : It will convert all the words in their roots.\n","\n","+ Example:\n","- loved as love\n","- helped as help\n","- hopes as hope\n","\n","+ Reason: \n","- As after cleaning the text when will create the bag of words model we will create sparse matrix with each column will have all the different words all having different emotions. So in order to optimize the dimension of the sparse matrix we need to apply stemming. If we don't apply the stemming then in sparse matrix we would have one column for present tense and other for the past tense that would be same thing so will create redundants and will make sparse matrix more complex with higher dimension. "]},{"cell_type":"code","execution_count":5,"id":"cfb60f0b","metadata":{"execution":{"iopub.execute_input":"2022-01-04T11:28:01.481244Z","iopub.status.busy":"2022-01-04T11:28:01.480535Z","iopub.status.idle":"2022-01-04T11:32:17.957833Z","shell.execute_reply":"2022-01-04T11:32:17.956847Z","shell.execute_reply.started":"2022-01-04T11:19:27.505835Z"},"papermill":{"duration":256.509273,"end_time":"2022-01-04T11:32:17.95803","exception":false,"start_time":"2022-01-04T11:28:01.448757","status":"completed"},"tags":[]},"outputs":[],"source":["from nltk.stem.porter import PorterStemmer\n","\n","# Cleaning the texts\n","# Creating empty list which will contain all the cleaned texts\n","# We will create a for loop to iterate all the texts of our datasets \n","# and for each of these review we will apply the cleaning process\n","# and after cleaning all the reviews we will add it into created empty list corpus\n","\n","Cleaned_Text = []\n","for i in range(len(dataset)):\n","    # Removing every punctuations and commas except a-z or A-Z by space\n","    Text = re.sub('[^a-zA-Z]', ' ', dataset['review'][i])\n","    \n","    # Transforming all the capital letters into lower case letters\n","    Text = Text.lower()\n","    \n","    # Splitting the text into different words so that we can apply stemming \n","    Text = Text.split()\n","    \n","    # Stemming the text\n","    ps = PorterStemmer()\n","    all_stopwords = stopwords.words('english')\n","    \n","    # Remove 'not' from the stopwords as it can alter the emotion of expression\n","    all_stopwords.remove('not')\n","    \n","    # Applying Stemming on all words except the stopwords\n","    Text = [ps.stem(word) for word in Text if not word in set(all_stopwords)]\n","    \n","    # Joining all the words together seperating with space\n","    Text = ' '.join(Text)\n","    \n","    # Adding the cleaned text to the empty list\n","    Cleaned_Text.append(Text)"]},{"cell_type":"markdown","id":"0fe8af3c","metadata":{"papermill":{"duration":0.021668,"end_time":"2022-01-04T11:32:18.001757","exception":false,"start_time":"2022-01-04T11:32:17.980089","status":"completed"},"tags":[]},"source":["### Cleaned_Text"]},{"cell_type":"code","execution_count":6,"id":"f7f31119","metadata":{"execution":{"iopub.execute_input":"2022-01-04T11:32:18.052608Z","iopub.status.busy":"2022-01-04T11:32:18.051866Z","iopub.status.idle":"2022-01-04T11:32:18.055514Z","shell.execute_reply":"2022-01-04T11:32:18.056315Z","shell.execute_reply.started":"2022-01-04T11:23:34.854661Z"},"papermill":{"duration":0.032377,"end_time":"2022-01-04T11:32:18.056595","exception":false,"start_time":"2022-01-04T11:32:18.024218","status":"completed"},"tags":[]},"outputs":[],"source":["# Cleaned_Text"]},{"cell_type":"markdown","id":"a8f2c36f","metadata":{"papermill":{"duration":0.021515,"end_time":"2022-01-04T11:32:18.101863","exception":false,"start_time":"2022-01-04T11:32:18.080348","status":"completed"},"tags":[]},"source":["### Creating the Bag of Words model"]},{"cell_type":"code","execution_count":7,"id":"0caf13b4","metadata":{"execution":{"iopub.execute_input":"2022-01-04T11:32:18.150337Z","iopub.status.busy":"2022-01-04T11:32:18.149596Z","iopub.status.idle":"2022-01-04T11:32:26.93697Z","shell.execute_reply":"2022-01-04T11:32:26.936407Z","shell.execute_reply.started":"2022-01-04T11:23:34.860734Z"},"papermill":{"duration":8.81169,"end_time":"2022-01-04T11:32:26.937126","exception":false,"start_time":"2022-01-04T11:32:18.125436","status":"completed"},"tags":[]},"outputs":[],"source":["from sklearn.feature_extraction.text import CountVectorizer\n","\n","# Creating instance of Count Vectorizer class\n","cv = CountVectorizer(max_features = 2000)\n","\n","# Fit method will take all the words \n","# and transform method will put all those in different columns\n","X = cv.fit_transform(Cleaned_Text).toarray()\n","\n","# Creating dependent variable\n","y1 = dataset.iloc[:, -1].values"]},{"cell_type":"markdown","id":"e0e98fad","metadata":{"papermill":{"duration":0.021147,"end_time":"2022-01-04T11:32:26.979702","exception":false,"start_time":"2022-01-04T11:32:26.958555","status":"completed"},"tags":[]},"source":["### Label encoding the dependent variables containing different sentiments"]},{"cell_type":"code","execution_count":8,"id":"f4e01657","metadata":{"execution":{"iopub.execute_input":"2022-01-04T11:32:27.045666Z","iopub.status.busy":"2022-01-04T11:32:27.032427Z","iopub.status.idle":"2022-01-04T11:32:27.049179Z","shell.execute_reply":"2022-01-04T11:32:27.048495Z","shell.execute_reply.started":"2022-01-04T11:23:43.162992Z"},"papermill":{"duration":0.047315,"end_time":"2022-01-04T11:32:27.049335","exception":false,"start_time":"2022-01-04T11:32:27.00202","status":"completed"},"tags":[]},"outputs":[],"source":["from sklearn.preprocessing import LabelEncoder\n","le = LabelEncoder()\n","y = le.fit_transform(y1)"]},{"cell_type":"code","execution_count":9,"id":"c0d9981f","metadata":{"execution":{"iopub.execute_input":"2022-01-04T11:32:27.10291Z","iopub.status.busy":"2022-01-04T11:32:27.102113Z","iopub.status.idle":"2022-01-04T11:32:27.107635Z","shell.execute_reply":"2022-01-04T11:32:27.106911Z","shell.execute_reply.started":"2022-01-04T11:23:43.186376Z"},"papermill":{"duration":0.036226,"end_time":"2022-01-04T11:32:27.107838","exception":false,"start_time":"2022-01-04T11:32:27.071612","status":"completed"},"tags":[]},"outputs":[{"data":{"text/plain":["array([1, 1, 1, ..., 0, 0, 0])"]},"execution_count":9,"metadata":{},"output_type":"execute_result"}],"source":["y"]},{"cell_type":"code","execution_count":10,"id":"4b50b7e1","metadata":{"execution":{"iopub.execute_input":"2022-01-04T11:32:27.169262Z","iopub.status.busy":"2022-01-04T11:32:27.158974Z","iopub.status.idle":"2022-01-04T11:32:27.174394Z","shell.execute_reply":"2022-01-04T11:32:27.173656Z","shell.execute_reply.started":"2022-01-04T11:23:43.203066Z"},"papermill":{"duration":0.044214,"end_time":"2022-01-04T11:32:27.174565","exception":false,"start_time":"2022-01-04T11:32:27.130351","status":"completed"},"scrolled":true,"tags":[]},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Sentiments</th>\n","      <th>Sentiments Encoded</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>positive</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>positive</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>positive</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>negative</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>positive</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>49995</th>\n","      <td>positive</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>49996</th>\n","      <td>negative</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>49997</th>\n","      <td>negative</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>49998</th>\n","      <td>negative</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>49999</th>\n","      <td>negative</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>50000 rows × 2 columns</p>\n","</div>"],"text/plain":["      Sentiments  Sentiments Encoded\n","0       positive                   1\n","1       positive                   1\n","2       positive                   1\n","3       negative                   0\n","4       positive                   1\n","...          ...                 ...\n","49995   positive                   1\n","49996   negative                   0\n","49997   negative                   0\n","49998   negative                   0\n","49999   negative                   0\n","\n","[50000 rows x 2 columns]"]},"execution_count":10,"metadata":{},"output_type":"execute_result"}],"source":["# Creating dataframe\n","Df1 = pd.DataFrame(y1)\n","Df1.columns = ['Sentiments']\n","Df1['Sentiments Encoded'] = y\n","Df1"]},{"cell_type":"markdown","id":"02da4e24","metadata":{"papermill":{"duration":0.022849,"end_time":"2022-01-04T11:32:27.219932","exception":false,"start_time":"2022-01-04T11:32:27.197083","status":"completed"},"tags":[]},"source":["### Splitting the datasets into training sets and the test sets"]},{"cell_type":"code","execution_count":11,"id":"cc3d4a62","metadata":{"execution":{"iopub.execute_input":"2022-01-04T11:32:27.273611Z","iopub.status.busy":"2022-01-04T11:32:27.271735Z","iopub.status.idle":"2022-01-04T11:32:27.619886Z","shell.execute_reply":"2022-01-04T11:32:27.619261Z","shell.execute_reply.started":"2022-01-04T11:23:43.223723Z"},"papermill":{"duration":0.376979,"end_time":"2022-01-04T11:32:27.620041","exception":false,"start_time":"2022-01-04T11:32:27.243062","status":"completed"},"tags":[]},"outputs":[],"source":["from sklearn.model_selection import train_test_split\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state = 42)"]},{"cell_type":"markdown","id":"7f94eb50","metadata":{"papermill":{"duration":0.021957,"end_time":"2022-01-04T11:32:27.664289","exception":false,"start_time":"2022-01-04T11:32:27.642332","status":"completed"},"tags":[]},"source":["# Creating Classification model"]},{"cell_type":"markdown","id":"ee5914e8","metadata":{"papermill":{"duration":0.021765,"end_time":"2022-01-04T11:32:27.708327","exception":false,"start_time":"2022-01-04T11:32:27.686562","status":"completed"},"tags":[]},"source":["### 1. Naive Bayes Classification Model"]},{"cell_type":"code","execution_count":12,"id":"d5da416f","metadata":{"execution":{"iopub.execute_input":"2022-01-04T11:32:27.760601Z","iopub.status.busy":"2022-01-04T11:32:27.759836Z","iopub.status.idle":"2022-01-04T11:32:29.586506Z","shell.execute_reply":"2022-01-04T11:32:29.585823Z","shell.execute_reply.started":"2022-01-04T11:23:43.563718Z"},"papermill":{"duration":1.855567,"end_time":"2022-01-04T11:32:29.586649","exception":false,"start_time":"2022-01-04T11:32:27.731082","status":"completed"},"tags":[]},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Predicted_Emotions</th>\n","      <th>Actual_Emotions</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>1</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>9995</th>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>9996</th>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>9997</th>\n","      <td>1</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>9998</th>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>9999</th>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>10000 rows × 2 columns</p>\n","</div>"],"text/plain":["      Predicted_Emotions  Actual_Emotions\n","0                      0                1\n","1                      1                1\n","2                      0                0\n","3                      1                1\n","4                      0                0\n","...                  ...              ...\n","9995                   0                0\n","9996                   0                1\n","9997                   1                1\n","9998                   0                0\n","9999                   0                1\n","\n","[10000 rows x 2 columns]"]},"execution_count":12,"metadata":{},"output_type":"execute_result"}],"source":["# Importing library for naive_bayes classification\n","from sklearn.naive_bayes import GaussianNB\n","\n","# Creating instance\n","classifier1 = GaussianNB()\n","\n","# Traning model as Naive_bayes classification model\n","classifier1.fit(X_train, y_train)\n","\n","# Predicting test sets results using Model\n","y_pred = classifier1.predict(X_test)\n","\n","# Comparing the predicted y and actual y to ensure accuracy of model\n","Df_1 = pd.DataFrame(y_pred)\n","Df_1.columns = ['Predicted_Emotions']\n","Df_1['Actual_Emotions'] = y_test\n","Df_1"]},{"cell_type":"code","execution_count":13,"id":"f11c829b","metadata":{"execution":{"iopub.execute_input":"2022-01-04T11:32:29.640596Z","iopub.status.busy":"2022-01-04T11:32:29.63955Z","iopub.status.idle":"2022-01-04T11:32:29.643046Z","shell.execute_reply":"2022-01-04T11:32:29.642528Z","shell.execute_reply.started":"2022-01-04T11:23:45.242281Z"},"papermill":{"duration":0.032288,"end_time":"2022-01-04T11:32:29.643245","exception":false,"start_time":"2022-01-04T11:32:29.610957","status":"completed"},"tags":[]},"outputs":[],"source":["# Importing library for creating Confusion matrix and accuracy\n","from sklearn.metrics import confusion_matrix, accuracy_score"]},{"cell_type":"code","execution_count":14,"id":"7ef4af5d","metadata":{"execution":{"iopub.execute_input":"2022-01-04T11:32:29.697502Z","iopub.status.busy":"2022-01-04T11:32:29.696751Z","iopub.status.idle":"2022-01-04T11:32:29.721084Z","shell.execute_reply":"2022-01-04T11:32:29.720292Z","shell.execute_reply.started":"2022-01-04T11:23:45.252312Z"},"papermill":{"duration":0.054292,"end_time":"2022-01-04T11:32:29.721259","exception":false,"start_time":"2022-01-04T11:32:29.666967","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["Confusion Matrix :\n","\n"," [[4228  733]\n"," [1840 3199]]\n","\n","R score =  0.7427\n"]}],"source":["# Creating Confusion matrix\n","cm = confusion_matrix(y_test, y_pred)\n","print('Confusion Matrix :\\n\\n',cm)\n","\n","# Finding R_score\n","R = accuracy_score(y_test, y_pred)\n","print('\\nR score = ',R)"]},{"cell_type":"code","execution_count":15,"id":"6e27d4c3","metadata":{"execution":{"iopub.execute_input":"2022-01-04T11:32:29.776894Z","iopub.status.busy":"2022-01-04T11:32:29.775911Z","iopub.status.idle":"2022-01-04T11:32:29.779367Z","shell.execute_reply":"2022-01-04T11:32:29.778787Z","shell.execute_reply.started":"2022-01-04T11:23:45.280467Z"},"papermill":{"duration":0.035325,"end_time":"2022-01-04T11:32:29.779513","exception":false,"start_time":"2022-01-04T11:32:29.744188","status":"completed"},"tags":[]},"outputs":[],"source":["def Prediction_Single_Text(Text):\n","    # Removing every punctuations and commas except a-z or A-Z by space\n","    Text = re.sub('[^a-zA-Z]', ' ', Text)\n","    \n","    # Transforming all the capital letters into lower case letters\n","    Text = Text.lower()\n","    \n","    # Splitting the text into different words so that we can apply stemming \n","    Text = Text.split()\n","    \n","    # Stemming the text\n","    ps = PorterStemmer()\n","    all_stopwords = stopwords.words('english')\n","    \n","    # Remove 'not' from the stopwords as it can alter the emotion of expression\n","    all_stopwords.remove('not')\n","    \n","    # Applying Stemming on all words except the stopwords\n","    Text = [ps.stem(word) for word in Text if not word in set(all_stopwords)]\n","    \n","    # Joining all the words together seperating with space\n","    Text = ' '.join(Text)\n","    \n","    Cleaned_Text = [Text]\n","    new_X_test = cv.transform(Cleaned_Text).toarray()\n","    new_y_pred1 = classifier1.predict(new_X_test)\n","    return new_y_pred1"]},{"cell_type":"code","execution_count":16,"id":"6782f808","metadata":{"execution":{"iopub.execute_input":"2022-01-04T11:32:29.8345Z","iopub.status.busy":"2022-01-04T11:32:29.83344Z","iopub.status.idle":"2022-01-04T11:32:29.841119Z","shell.execute_reply":"2022-01-04T11:32:29.840462Z","shell.execute_reply.started":"2022-01-04T11:23:45.289685Z"},"papermill":{"duration":0.03685,"end_time":"2022-01-04T11:32:29.841317","exception":false,"start_time":"2022-01-04T11:32:29.804467","status":"completed"},"tags":[]},"outputs":[{"data":{"text/plain":["array([0])"]},"execution_count":16,"metadata":{},"output_type":"execute_result"}],"source":["Text = 'I wasted my money in this movie.'\n","Prediction_Single_Text(Text)"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.12"},"papermill":{"default_parameters":{},"duration":303.858361,"end_time":"2022-01-04T11:32:30.779284","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2022-01-04T11:27:26.920923","version":"2.3.3"}},"nbformat":4,"nbformat_minor":5}